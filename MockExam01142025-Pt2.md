Mock Exam 1

Section 1: Design Applications

A Generative AI Engineer is tasked with designing an LLM-powered assistant to create weekly business reports with a specific format. What is the most critical step in creating a prompt for this task?
A. Adding examples of poorly formatted reports
B. Including specific instructions about the structure and required sections of the report
C. Choosing a model with high perplexity
D. Setting a random temperature value

Which of the following best translates a business requirement into an AI pipeline input/output description?
A. Randomly selecting inputs from user feedback
B. Writing a detailed description of the inputs and outputs based on user requirements
C. Selecting pre-defined templates without modification
D. Ignoring edge cases

Section 2: Data Preparation

Which Python library is best suited for extracting text from a PDF document to prepare for chunking?
A. Numpy
B. PyPDF2
C. Matplotlib
D. Seaborn

In a Retrieval-Augmented Generation (RAG) system, what is the primary reason to filter out extraneous content from documents?
A. To reduce the file size
B. To improve response latency
C. To enhance retrieval quality and relevance
D. To increase the complexity of the application

Section 3: Application Development

How can a Generative AI Engineer prevent a chatbot from producing inappropriate outputs?
A. Avoiding guardrails and using the default model
B. Implementing LLM guardrails to filter and monitor outputs
C. Increasing the model’s training epochs
D. Reducing the temperature of the model to zero

Which prompt adjustment technique ensures an LLM-generated response aligns closely with user requirements?
A. Using a shorter prompt
B. Including key fields and intents in the prompt context
C. Avoiding any context augmentation
D. Randomly selecting additional context

Section 4: Assembling and Deploying Applications

Which step is essential for creating a functional RAG application?
A. Using random dependencies
B. Selecting a retriever and embedding model based on application requirements
C. Avoiding the use of a model signature
D. Ignoring embedding model context length

How can a Generative AI Engineer create a basic chain using LangChain?
A. By writing a standalone function
B. By integrating pre- and post-processing steps using LangChain utilities
C. By deploying the model without any chain components
D. By avoiding any configuration of the chain

Section 5: Governance

What guardrail technique is most effective for protecting against malicious inputs in a Generative AI application?
A. Ignoring user input validation
B. Using input masking and validation techniques
C. Fine-tuning the model only
D. Allowing unrestricted user inputs

Which approach ensures compliance with data source licensing for a RAG application?
A. Ignoring the data’s legal requirements
B. Reviewing and adhering to legal/licensing agreements
C. Using any available data without checks
D. Selecting data at random

Section 6: Evaluation and Monitoring

When monitoring the performance of a deployed RAG application, which metric is most indicative of retrieval quality?
A. Token count
B. Latency
C. Retrieval accuracy
D. Model perplexity

How can a Generative AI Engineer use Databricks features to control costs for a deployed LLM?
A. Increase the number of endpoints
B. Use cost monitoring tools and limit token usage
C. Deploy a larger model
D. Disable all cost controls

Mock Exam 2

Section 1: Design Applications

When designing a multi-stage reasoning AI application, what is the most critical step in tool selection?
A. Selecting tools randomly
B. Defining and ordering tools to gather relevant knowledge for each stage
C. Using only pre-built tools
D. Ignoring tool compatibility

Which chain component is most critical for transforming user input into actionable model outputs?
A. Data redundancy
B. Appropriate pre-processing steps
C. Ignoring user input formatting
D. Randomized model tasks

Section 2: Data Preparation

What is the first step in applying a chunking strategy to a large document?
A. Extracting relevant sections based on model constraints
B. Randomly splitting the document into smaller parts
C. Avoiding chunking to maintain document integrity
D. Ignoring the document structure

How can extraneous data negatively affect a RAG application?
A. It improves response variety
B. It degrades the accuracy of generated responses
C. It speeds up retrieval processes
D. It enhances model hallucination capabilities

Section 3: Application Development

How can you adjust an LLM’s response from a baseline to a desired output?
A. Ignoring the baseline response
B. Fine-tuning the prompt to include desired outcome specifics
C. Increasing the randomness of the model’s outputs
D. Disabling all context augmentation

What is the purpose of metaprompting in LLM applications?
A. To increase the likelihood of hallucinations
B. To minimize hallucinations and protect private data
C. To generate irrelevant outputs
D. To complicate the user’s prompt unnecessarily

Section 4: Assembling and Deploying Applications

What is the key step in deploying an endpoint for a RAG application?
A. Ignoring model signature and dependencies
B. Registering the model in Unity Catalog
C. Avoiding embedding model selection
D. Randomly sequencing deployment steps

Which method improves query performance in Vector Search?
A. Using fewer indexing strategies
B. Creating an optimized Vector Search index
C. Disabling embeddings
D. Using an outdated retriever

Section 5: Governance

How can a Generative AI Engineer prevent legal risks when using external data sources?
A. Ignoring licensing requirements
B. Ensuring data sources comply with licensing agreements
C. Randomly selecting data sources
D. Avoiding validation of data sources

Which technique protects sensitive user data in Generative AI outputs?
A. Masking sensitive fields in the input and output
B. Ignoring user input security
C. Allowing unrestricted user input access
D. Removing all guardrails

Section 6: Evaluation and Monitoring

How can inference logging improve the monitoring of a deployed RAG application?
A. By reducing the frequency of data updates
B. By providing insights into request and response patterns
C. By disabling logging features
D. By ignoring performance metrics

Which metric is crucial for evaluating the cost-effectiveness of an LLM deployment?
A. Model size
B. Token usage and throughput costs
C. Length of input text
D. Random latency values
