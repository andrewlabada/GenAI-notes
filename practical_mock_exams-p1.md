# Databricks Gen AI Engineer Associate Mock Exams

## Section 1: Design Applications

### Mock Exam 1

1. Design a prompt to extract customer feedback from a product review dataset in JSON format.
2. Identify the model tasks necessary to analyze customer sentiment and detect product features mentioned.
3. Choose the chain components required for processing the input and generating a tabular summary of insights.
4. Translate the business goal of identifying trending product issues into the AI pipeline’s required inputs and outputs.
5. Define the tools needed to gather customer feedback and summarize results effectively.

### Mock Exam 2
1. Design a prompt that generates structured financial summaries from unstructured transaction logs.
2. Select the appropriate model tasks for summarizing financial trends and flagging anomalies.
3. Propose a sequence of chain components for cleaning, processing, and reporting financial data.
4. Translate the requirement to minimize financial discrepancies into actionable AI pipeline steps.
5. Determine tools to perform multi-stage reasoning for anomaly detection.

## Section 2: Data Preparation

### Mock Exam 1

1. Given a set of long scientific papers, apply a chunking strategy to optimize their ingestion by an AI model with a token limit of 4,096 tokens.
2. Propose filtering criteria for irrelevant sections of a document to improve response quality in a RAG application.
3. Select a Python package suitable for extracting text from PDFs with embedded images.
4. Define a sequence of operations to write chunked text into Delta Lake tables.
5. Identify the types of source documents required for accurate responses in a medical diagnosis application.


## Section 3: Application Development

### Mock Exam 1

1. Write a Python script to extract specific columns from a CSV file for a data retrieval need.
2. Choose the best LangChain components for building a conversational agent for a retail chatbot.
3. Identify how varying prompt formats can lead to different model outputs for query refinement.
4. Review a model’s response to customer inquiries and suggest quality and safety improvements.
5. Propose a chunking strategy that optimizes retrieval performance for a real estate search application.

### Mock Exam 2

1. Augment a basic prompt with context derived from user inputs to enhance query specificity.
2. Create a metaprompt designed to mitigate hallucinations in historical document analysis.
3. Implement guardrails in a generative AI pipeline to prevent generation of harmful or biased content.
4. Select an embedding model with a suitable context length for a technical documentation repository.
5. Recommend the best model from a marketplace based on metadata and a given task scenario.


## Section 4: Assembling and Deploying Applications

### Mock Exam 1

1. Code a basic pyfunc model with pre- and post-processing steps for text summarization.
2. Define access control strategies for model-serving endpoints in a cloud environment.
3. Develop a simple LangChain chain to extract answers to user queries from a knowledge base.
4. List the elements required to build a RAG application for e-commerce recommendations.
5. Sequence the steps to deploy a conversational AI endpoint using Unity Catalog.

### Mock Exam 2

1. Build a Vector Search index for document retrieval and write a Python script to query it.
2. Identify resources necessary to enable feature serving in a RAG application for healthcare.
3. Propose deployment strategies for a multi-model LLM application using Foundation Model APIs.
4. Explain the steps to register a pre-trained model in Unity Catalog with relevant metadata.
5. Identify key dependencies for assembling a recommendation system pipeline.


## Section 5: Governance

### Mock Exam 1

1. Propose masking techniques to meet a performance objective while safeguarding user privacy.
2. Select guardrails that prevent malicious input from disrupting a generative AI application.
3. Recommend alternative techniques for problematic text mitigation in a news aggregation pipeline.
4. Suggest best practices to comply with legal and licensing requirements for a dataset.

### Mock Exam 2

1. Design a governance framework for mitigating risks in deploying a generative chatbot for legal advice.
2. Identify key legal considerations for using copyrighted content in AI training datasets.
3. Propose techniques for anonymizing sensitive information in a medical dataset.
4. Recommend mitigation strategies for handling offensive content in user-generated inputs.


## Section 6: Evaluation and Monitoring

### Mock Exam 1

1. Select the optimal LLM size for a translation application based on latency and quality metrics.
2. Identify key metrics to monitor during the deployment of an LLM-powered customer service agent.
3. Evaluate the performance of a RAG application for retrieving historical documents using MLflow.
4. Propose methods for logging inference data to monitor system performance over time.
5. Recommend cost-saving features for controlling LLM usage in a production environment.

### Mock Exam 2

1. Assess quantitative evaluation metrics for model selection in a multi-lingual document summarization application.
2. Design a monitoring dashboard for tracking errors and latency in a real-time LLM deployment.
3. Propose methods to evaluate retrieval performance for a hybrid search and generation application.
4. Identify features in Databricks that help optimize compute costs for serving generative applications.
